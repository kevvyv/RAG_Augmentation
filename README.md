# RAG_Augmentation

Context Boosting Retrieval Augmented Generation
 ________________________________________
 Kevin Yu
 Machine Learning Student
 University of California San Diego
 key010@ucsd.edu
 Abstract
 Retrievieval Augmented Generation (RAG) is an innovative and efficient architecture that
 has recently gained traction in large language model systems. The RAG aims to provide a
 great optimization in allowing the LLM to retrieve factual and accessible information
 when appropriate, while relying on its standard generations for normal cases. In this paper,
 I will delve into the structure and performance of the standard RAG and note the areas that
 can be improved upon. I then propose a methodology of integrating hierarchical clustering
 based context boosting for queries as well as using a graphical representation of data to
 improve response accuracy on specific queries. These will then be compared with the
 standard RAG performance and the results will be analyzed.
 1 Introduction
 The retrieval augmented generation framework, at its core, is a way to supplement a query to an LLM and
 achieve a relevant response based on the users query and this use case can immediately be extended into
 establishing a language model capable of understanding internal company/user information without the
 need to go through the complicated of process of fine tuning. It is composed of 3 main components.
 Firstly, all relevant data that one would want their RAG to be able to interact and query from, will be
 embedded (with an embedding model of the individual's choice) into a vector store. This process involves
 segmenting the inputted documents into text chunks that fit within the maximum token limit of the chosen
 embedding model. Next, the segments are tokenized and converted into a sequence embedding
 representation. Embeddings are used to represent and capture the semantic meaning of tokens (can be
 thought of as words) and the embedding models that are used, have been trained to learn how to represent
 words as dense vectors that contain contain rich contextual information about the word (note: something
 very powerful about this is that the models can observe how the word is being used in the specific
 context/sentence it is in and have a unique embedding be generated relevant to the sequence it is from).
 Nowthat it is clear how we can reliably capture the semantic meaning of all our documents, the vector
 store simply provides a structure where documents and their embeddings can be reliably compared and
 stored with designated index retrieval and storage methods. After storing and embedding our documents,
 the next step of the architecture requires the user to now input their question they would like the large
 language model to respond to. The input question will be embedded in the same way the documents were
and from there, a similarity search, usually based on cosine similarity score or keyword matching, is
 performed over the vector store and relevant documents are returned. These contextual similar documents
 are then used as supplementation when passing the question towards the LLM where the language model
 can now view the relevant documents and respond to the question accordingly. This method works
 reliably for simple and straightforward based queries. However, when we would like to have a more
 expansive and detailed response to our queries, as well as when presenting complex query questions that
 require a very specific answer, we see the standard approach begin to falter. This can best be seen with
 two main examples highlighting the weaknesses.
 Figure 1
 I have set up a RAG with respect to Figure 1 and the documents (dataset) I had chosen to embed, were
 documents taken from PDFs of Professor Tu’s website, various UCSD official webpages, as well as
 documents from the CSE department course catalogs.
 Figure 2
 Figure 3
 When experimenting with various queries, I found that broad topics and general questions were well
 suited for the RAG system. However, seen in figure 2 and figure 3, These are where I noted areas to
 improve upon. I had posed the question “Tell me about Professor Tu’s lab”. I then received a brief
 summary of 3 sentences covering (can be seen in figure 2). While this is a sufficient introduction, I
 believed it was possible for a more indepth and detailed explanation to be generated by the
 supplementation of more contextually relevant documents. The more glaring weakness however, was
 when I had asked “What are the prerequisites for CSE100”. The response can be seen in Figure 3 and it
 can be noted that the model got the response blatantly wrong. This was most likely due to the inability to
 focus on precise details as when seeing the entire catalog with an excess amount of course descriptions
 and prerequisites, it can be hard for the model to attend to the pinpoint segment dedicated to CSE100.
 Thus I will now introduce the methodology I propose for tackling these areas.
2 Methodology
 To introduce the architecture I implemented, Figure 4 can be referenced for an overarching perspective.
 The first notable weakness that was mentioned previously was the desire for more information and detail
 in the responses and to accomplish this, the system needs to be retrieving an adequate amount of relevant
 documents. When using Llama index’s baseline structure, the query is retrieving 1-2 documents at the
 maximum, which relate to the query. While this provides a concise and relevant response, it perhaps
 places too much of a restriction on how much information we supply the model with. With the large token
 window of LLMs such as GPT 3.5- turbo (the model being used for this paper), we can assuredly add
 more relevant information to give the language model more information to generate its response on.
 Figure 4
 To implement the mechanism where additional relevant documents are retrieved, I have decided to run an
 embedding and clustering process separate from the Llama index vector store, which will still be used in
 tandem with the llama index retrieval documents to provide a boosted supply of relevant documents for
 the LLM. Following similarly from how documents are embedded, I segmented the respective documents
 and ran the embedding model on the documents to achieve word embeddings for every document
 segment. From there, a mean pooling was applied to each segment's embeddings so that in the end, there
 was a single concise sequence embedding to represent each individual document segment (note that the
 document segment indexes are stored as well to pair them back with their original text when feeding into
 the model). Once the documents have been embedded, I ran agglomerative clustering (using cosine
 similarly to compare documents) to hierarchically cluster the data. The motive behind this is to
 understand that the result of hierarchically clustering the data would be a dendrogram of the entire
 document dataset, from there, when we have a query, we can pinpoint its location in the dendrogram and
 explore a radius of 1 edge above and below to effectively get its most relevant neighbors as the
 dendrogram structure is created such that those closest to the node (document) are the most relevant. In
 this way, when a query is called, we can immediately reference our clusters and return an additional
 supply of relevant documents and append them to the llama index query before the model is prompted.
 Thus, effectively adding additional context within each query.
To address the more prevalent weakness of the system's lack of ability to precisely identify prerequisite
 courses in the model responses, I have researched modern methods in which researchers are aiming to
 improve RAGs and a current area of exploration is graph augmentation. While not explicitly creating a
 direct graph that the LLM will have connections with, this served as inspiration for an idea of how to
 structure data better for the LLM to interact with during the prompting process. I created a feature point
 where if certain relevant keywords are detected ("cse", "prerequisites", "classes", 'cogs', 'computer
 science','courses' etc), then the model will add a special supplement to the query which is an adjacency list
 representation of a directed acyclic graph of the courses. By providing the model with this concise and
 practical representation of the data along with the original query system, the model is able to redirect its
 focus to the easy to interpret data structure when the relevant query is asked and thus, a hopefully
 sufficient augmentation for more specific query cases.
 3 Experiment
 To observe and discuss the results of the implemented augmentations, we can reference figure 5, 6 and 7
 which are the respective outputs for the same questions that were asked in the base case example. When
 comparing the output of figure 5 to figure 2, we can see the sizable increase in detail where the
 contextually boosted response is able to provide an overall summary as well as present many specific
 examples of Professor Tu’s research that are much more complex than the original in figure 1. Regarding
 figure 6, we can observe that the model was able to correctly identify the prerequisites of CSE100 due to
 the adjacency list supplementation and when testing it on various other courses, it was able to correctly
 identify prerequisites for all those additions as well. Figure 7 is included to show that while the graph
 augmentation allows the model to now provide accurate responses for specific courses, the graph never
 becomes overbearing for the model as when the model is provided the graph on a question of “What is
 CSE100 about?”, it is still able to respond accordingly and ignore the graph when it would not be useful
 to answering the query. It is clear that there were notable improvements in both aspects that were tested
 for and thus, these represent some promising results to further explore these two avenues of RAG
 modification.
4 Conclusion
 To conclude, it is evident that the RAG as a standard framework alone, is a powerful structure that can
 effectively be applied to any document base; however, it comes with its limitations in context retrieval
 and attention to detail. However, when applying a methodology to counteract these weaknesses, it has
 opened the door for all new possibilities and imaginings of other improvements that can be made to RAG
 and thus, is what makes this new field so exciting with the limitless directions of innovations to be made.
 If I had to continue the project, I would search for a more standardized RAG benchmarking dataset, to
 then test these methodologies in a more formal manner. I have found the exploration of this project to
 truly be a meaningful and informative experience in pursuing research as well as being exposed to such a
 prevalent area in NLP today.
 5 References
 P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W. Yih, T.
 Rocktäschel, S. Riedel, and D. Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks.
 Facebook AI Research; University College London; New York University
 Ontotext. (n.d.). What is Graph RAG? Retrieved from
 https://www.ontotext.com/knowledgehub/fundamentals/what-is-graph-rag
